{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import packages\" data-toc-modified-id=\"Import packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import packages</a></span>\n",
    "   \n",
    "\n",
    "<li><span><a href=\"#Import datasets\" data-toc-modified-id=\"Import datasets-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import datasets</a></span>\n",
    "\n",
    "<li><span><a href=\"#Train-test split-3\" data-toc-modified-id=\"Train-test split-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train-test split</a></span>\n",
    "    \n",
    "\n",
    "<li><span><a href=\"#Transform data-Tokenize\" data-toc-modified-id=\"Transform data-Tokenize-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Transform data-Tokenize</a></span>\n",
    "    \n",
    "<li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Modeling</a></span>    \n",
    "    \n",
    "<ul class=\"toc-item\">\n",
    "\n",
    "<li><span><a href=\"#Train model\" data-toc-modified-id=\"Train model-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Real-News Dataset</a></span>\n",
    "    \n",
    "</li><li><span><a href=\"#Test model-4.2\" data-toc-modified-id=\"Test model-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Test model</a></span></li>\n",
    "\n",
    "</li><li><span><a href=\"#Test model on scrapped data\" data-toc-modified-id=\" Test model on scrapped data-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Test model on scrapped data</a></span></li>\n",
    "\n",
    "</ul></li>\n",
    "\n",
    "<li><span><a href=\"Conclusions\" data-toc-modified-id=\"Conclusions-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusions</a></span>\n",
    " \n",
    "</ul></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### Import necessary libraries & packages ###\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#from sklearn.model_selection import train_test_split,cross_val_score\n",
    "\n",
    "# Our custom functions #\n",
    "#%run clf_functions.ipynb\n",
    "\n",
    "from nltk import word_tokenize\n",
    "#from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON (Reuters) - The head of a conservative Republican faction in the U.S. Congress, who voted this month for a huge expansion of the national debt to pay for tax cuts, called himself a “fiscal conservative” on Sunday and urged budget restraint in 2018. In keeping with a sharp pivot under way among Republicans, U.S. Representative Mark Meadows, speaking on CBS’ “Face the Nation,” drew a hard line on federal spending, which lawmakers are bracing to do battle over in January. When they return from the holidays on Wednesday, lawmakers will begin trying to pass a federal budget in a fight likely to be linked to other issues, such as immigration policy, even as the November congressional election campaigns approach in which Republicans will seek to keep control of Congress. President Donald Trump and his Republicans want a big budget increase in military spending, while Democrats also want proportional increases for non-defense “discretionary” spending on programs that support education, scientific research, infrastructure, public health and environmental protection. “The (Trump) administration has already been willing to say: ‘We’re going to increase non-defense discretionary spending ... by about 7 percent,’” Meadows, chairman of the small but influential House Freedom Caucus, said on the program. “Now, Democrats are saying that’s not enough, we need to give the government a pay raise of 10 to 11 percent. For a fiscal conservative, I don’t see where the rationale is. ... Eventually you run out of other people’s money,” he said. Meadows was among Republicans who voted in late December for their party’s debt-financed tax overhaul, which is expected to balloon the federal budget deficit and add about $1.5 trillion over 10 years to the $20 trillion national debt. “It’s interesting to hear Mark talk about fiscal responsibility,” Democratic U.S. Representative Joseph Crowley said on CBS. Crowley said the Republican tax bill would require the  United States to borrow $1.5 trillion, to be paid off by future generations, to finance tax cuts for corporations and the rich. “This is one of the least ... fiscally responsible bills we’ve ever seen passed in the history of the House of Representatives. I think we’re going to be paying for this for many, many years to come,” Crowley said. Republicans insist the tax package, the biggest U.S. tax overhaul in more than 30 years,  will boost the economy and job growth. House Speaker Paul Ryan, who also supported the tax bill, recently went further than Meadows, making clear in a radio interview that welfare or “entitlement reform,” as the party often calls it, would be a top Republican priority in 2018. In Republican parlance, “entitlement” programs mean food stamps, housing assistance, Medicare and Medicaid health insurance for the elderly, poor and disabled, as well as other programs created by Washington to assist the needy. Democrats seized on Ryan’s early December remarks, saying they showed Republicans would try to pay for their tax overhaul by seeking spending cuts for social programs. But the goals of House Republicans may have to take a back seat to the Senate, where the votes of some Democrats will be needed to approve a budget and prevent a government shutdown. Democrats will use their leverage in the Senate, which Republicans narrowly control, to defend both discretionary non-defense programs and social spending, while tackling the issue of the “Dreamers,” people brought illegally to the country as children. Trump in September put a March 2018 expiration date on the Deferred Action for Childhood Arrivals, or DACA, program, which protects the young immigrants from deportation and provides them with work permits. The president has said in recent Twitter messages he wants funding for his proposed Mexican border wall and other immigration law changes in exchange for agreeing to help the Dreamers. Representative Debbie Dingell told CBS she did not favor linking that issue to other policy objectives, such as wall funding. “We need to do DACA clean,” she said.  On Wednesday, Trump aides will meet with congressional leaders to discuss those issues. That will be followed by a weekend of strategy sessions for Trump and Republican leaders on Jan. 6 and 7, the White House said. Trump was also scheduled to meet on Sunday with Florida Republican Governor Rick Scott, who wants more emergency aid. The House has passed an $81 billion aid package after hurricanes in Florida, Texas and Puerto Rico, and wildfires in California. The package far exceeded the $44 billion requested by the Trump administration. The Senate has not yet voted on the aid. \n"
     ]
    }
   ],
   "source": [
    "#load the data and turn it into a dataframe.\n",
    "df_true = pd.read_csv('True.csv')\n",
    "df_true.columns\n",
    "\n",
    "#df_true = df_true[['label', 'Clean_text']]\n",
    "df_true['label'] = 'true'\n",
    "df_true = df_true[['text','label']]\n",
    "df_true.head()\n",
    "print(df_true['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.\n"
     ]
    }
   ],
   "source": [
    "#load the data and turn it into a dataframe.\n",
    "df_fake = pd.read_csv('Fake.csv')\n",
    "df_fake.columns\n",
    "\n",
    "#df_true = df_true[['label', 'Clean_text']]\n",
    "df_fake['label'] = 'fake'\n",
    "df_fake = df_fake[['text','label']]\n",
    "print(df_fake.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44898"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_fake, df_true], axis = 0, ignore_index=True )\n",
    "df.reset_index()\n",
    "len(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the column that will be the target, 'label', in the variable target.\n",
    "target = df['label']\n",
    "#target.head()\n",
    "#df['text'].head(2)\n",
    "#Store the 'text' column in the variable data.\n",
    "data = df['text'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size = 0.20, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'fit_on_texts' -  Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
    "\n",
    "'texts_to_sequences' -  Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. \n",
    "\n",
    "'pad_sequences' - This function transforms a list of num_samples sequences (lists of integers) into a matrix of shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, or the length of the longest sequence otherwise.\n",
    "\n",
    "Sequences that are shorter than num_timesteps are padded with value at the end.\n",
    "\n",
    "Sequences longer than num_timesteps are truncated so that they fit the desired length. The position where padding or truncation happens is determined by the arguments padding and truncating, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data - Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "#limit vocabulary to 2000.\n",
    "tokenizer = Tokenizer(num_words = 2000, split = ' ')\n",
    "tokenizer.fit_on_texts(X_train.astype(str).values)\n",
    "train_text = tokenizer.texts_to_sequences(X_train.astype(str).values)\n",
    "max_len = max([len(i) for i in train_text])\n",
    "train_text = pad_sequences(train_text, maxlen = max_len)\n",
    "test_text = tokenizer.texts_to_sequences(X_test.astype(str).values)\n",
    "test_text = pad_sequences(test_text, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   48, 1661,  292],\n",
       "       [   0,    0,    0, ...,   42, 1423,   56],\n",
       "       [   0,    0,    0, ..., 1928,    5,  743],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  865,  210,  148],\n",
       "       [   0,    0,    0, ...,   10, 1790,  120],\n",
       "       [   0,    0,    0, ...,   92,  245, 1157]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 6204, 128)         256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 6204, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 650,754\n",
      "Trainable params: 650,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#construct the neural network\n",
    "model = Sequential()\n",
    "model.add(Embedding(2000, 128, input_length = train_text.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(256, dropout = 0.2))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28734 samples, validate on 7184 samples\n",
      "Epoch 1/3\n",
      "28734/28734 [==============================] - 8859s 308ms/step - loss: 0.1717 - acc: 0.9383 - val_loss: 0.0990 - val_acc: 0.9628\n",
      "Epoch 2/3\n",
      "28734/28734 [==============================] - 32787s 1s/step - loss: 0.0560 - acc: 0.9828 - val_loss: 0.0358 - val_acc: 0.9890\n",
      "Epoch 3/3\n",
      "28734/28734 [==============================] - 24993s 870ms/step - loss: 0.0387 - acc: 0.9898 - val_loss: 0.0216 - val_acc: 0.9940\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit(train_text, pd.get_dummies(y_train).values, epochs = 3, batch_size = 128, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8980/8980 [==============================] - 411s 46ms/step\n",
      "Test accuracy: 0.995991\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "score, accuracy = model.evaluate(test_text, pd.get_dummies(y_test).values, batch_size = 128)\n",
    "print(\"Test accuracy: {0:.6f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model on scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_guardian = pd.read_csv('the_guardian_file.csv')\n",
    "\n",
    "df_guardian = df_guardian[['text']]\n",
    "df_guardian['label'] = '1'\n",
    "print(df_guardian.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Store the column that will be the target, 'Sentiment', in the variable target.\n",
    "target_guardian = df_guardian['label']\n",
    "#target.head()\n",
    "#df['text'].head(2)\n",
    "#Store the Clean_tweet column in the variable data.\n",
    "data_guardian = df_guardian['text'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size = 0.20, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "#limit vocabulary to 2000.\n",
    "tokenizer = Tokenizer(num_words = 2000, split = ' ')\n",
    "tokenizer.fit_on_texts(X_train.astype(str).values)\n",
    "train_text = tokenizer.texts_to_sequences(X_train.astype(str).values)\n",
    "max_len = max([len(i) for i in train_text])\n",
    "train_text = pad_sequences(train_text, maxlen = max_len)\n",
    "test_text = tokenizer.texts_to_sequences(X_test.astype(str).values)\n",
    "test_text = pad_sequences(test_text, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score, accuracy = model.evaluate(test_text, pd.get_dummies(target_guardian).values, batch_size = 128)\n",
    "print(\"Test accuracy: {0:.6f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add to our scrapped data fake articles in order to test the model. The output layer has to be 2 but in our case is one because all the articles are considered truthfull.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Future work would be to tune the model because an accuracy of 0.99 is too good to be thrue and add some fake articles to test the model on scrapped data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
